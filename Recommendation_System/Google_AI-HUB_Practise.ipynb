{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Google_AI-HUB_Practise.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andi23/Custlr-AI/blob/Zobaid_branch/Recommendation_System/Google_AI-HUB_Practise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCGLKq4hBed8",
        "colab_type": "text"
      },
      "source": [
        "# Introduction of Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5srLSkBeeC",
        "colab_type": "text"
      },
      "source": [
        "With the rise of internet shopping and online services, there has been a motivation to create systems that can predict which items a user would like. This can allow for a better user experience, and can allow a seller to present items that a user would be more likely to purchase. For example. a company like Amazon wants to show you relevant items that you will consider. \n",
        "\n",
        "A class of such systems are called \"collaborative filtering\". They analyze data between user and items to come up with previosuly hidden relationships that can be used to better reccomend items to users. Matrix Factorization is a specific type of collabroative fitering method that in a general sense tries to approximate sub-matricies that are equivalent to a larger matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjD7a_QOBeeE",
        "colab_type": "text"
      },
      "source": [
        "Matrix factorization (MF) grew in popularity in part of the netflix prize dataset (which we will be using in this article), but also because of the sparsity and expanse of data being collected by companies. A company such as Netflix has millions of users and maybe millions of movies. Each user has not viewed and rated each movie - in fact they have likely viewed very few movies of the overall set. If we were to simply create a matrix based on a users for each row and movies for each column, there would be a gigantic sparse matrix. Most collabrorative filtering algorithms would have issues dealing with such sparse matricies, but MF is uniquely equipped to deal with this issue. Below we see an objective function detailing how we come up with our smaller sub-matricies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OntY76tBeeG",
        "colab_type": "text"
      },
      "source": [
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.1/kfp-components/notebooks/recommender_system/assets/explain_img_2.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6smTywEBeeI",
        "colab_type": "text"
      },
      "source": [
        "The basis of Matrix factorization goes a little bit like this: say you have a sparse matrix A that consists of rows defining users and colums defining items for purchase. Each element of this matrix will be the rating given from a user to a movie. For example, let's say user 1 really liked 'Fast & Furious': in the row for user 1 and the row for 'Fast & Furious' we will see the rating given by user 1 to the movie. Repeat this process for every user and every item that the user has rated. \n",
        "\n",
        "Now what if we would create smaller matricies B, C whose multiplication would give us back all the elements of A? In this way, we wouldn't need to keep a gigantic sparse matrix on file, and could rely on updating these smaller matricies.  This would mean that B * C = A. B and C are both built by creating what are known as \"latent features\". Each user and item will have a chosen number of features, explaining hidden relationships between users and items. \n",
        "\n",
        "Going back to the Fast & Furious and user 1 example, let's create some hypothetical latent features. User 1 may really like action movies and comedy movies. Internally, Fast and Furious may have latent features showing that it is an action adventure movie with some comedy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6A7--bOBeeJ",
        "colab_type": "text"
      },
      "source": [
        "Below we see an illustrated example of hypothethical latent features in a movie case. There are 5 movies and 4 users. Each movie has a score of how much comedy or action it contains (a score from 1 to 5), and each user has a score of whether they like comedy or action (a check mark can be considered 1 and an 'X' can be considered to be 0). We can use the smaller matricies to generate the larger matrix. As example we can take the dot product of the first row of the user matrix (1,0) and the first column of the movie matrix (3,1) to get the movie rating for movie 1 by user 1 to be 3. ($ 1*3 + 0*1 = 3$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrbPemXyBeeL",
        "colab_type": "text"
      },
      "source": [
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.1/kfp-components/notebooks/recommender_system/assets/explain_img_1.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GowhOQ1rBeeM",
        "colab_type": "text"
      },
      "source": [
        "Matrix factorization can be used in domains that contain the following things [4]: \n",
        "- Many Users\n",
        "- Many Items\n",
        "- Many Ratings\n",
        "- Users rate multiple items\n",
        "- For each user of the community, there are other users with common needs or tastes\n",
        "- Item evaluation requires personal taste\n",
        "- Items persists\n",
        "- Taste persists\n",
        "- Items are homogenous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8scmU1VoBeeO",
        "colab_type": "text"
      },
      "source": [
        "As an example of the space saving power of Matrix Factorization, let's look at the following example where we have 4,500 items and 470,000 users (as we will in this notebook). If we create a sparse matrix we would have 2.1 Billion enteries. If we use matrix factorization we will have 4.7M + 45K entries. This is approximately a space savings of over 400x! This means that if we just use the submatrices to store information about users and items, we can save 400x the disk space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erh8ASt8BeeP",
        "colab_type": "text"
      },
      "source": [
        "![](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.1/kfp-components/notebooks/recommender_system/assets/explain_img_3.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHZmmIrJBeeR",
        "colab_type": "text"
      },
      "source": [
        "Matrix factorization is not a silver bullet however, there are limilations and problems with this method. One of the biggest problems is the \"cold start\". This is when a new user or item is added to the system, and there is not enough information to give ratings with a new user, and the initial suggestions are often incorrect -- we can't fill in the blank. Also, if certain users have unusual interests (with respect to the rest of the users), it can cause errors in inference. [5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8xO2gaqBeeS",
        "colab_type": "text"
      },
      "source": [
        "# This Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Epsqnn3BeeU",
        "colab_type": "text"
      },
      "source": [
        "This notebook will go over : \n",
        "\n",
        "1. Downloading user and item data\n",
        "2. Donwloading offical Tensorflow model dependacies. \n",
        "3. Creating a pandas dataframe \n",
        "4. Creating tf.keras model\n",
        "5. Converting dataframe to loadable data\n",
        "6. Training model\n",
        "7. Evaluating model and finding metrics \n",
        "8. Visualizing model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opO55lIBeeV",
        "colab_type": "text"
      },
      "source": [
        "# The Data: Netflix Prize Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBVLOgABBeeX",
        "colab_type": "text"
      },
      "source": [
        "Netflix during the Nextflix Prize competition provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies. Each training rating is a quadruplet of the form user, movie, date of grade, grade. The user and movie fields are integer IDs, while grades are from 1 to 5 stars. The data used in this article was downloaded from Kaggle, in the form of 4 text files. In this notebook, we will only be using 1 of the 4 data files consiting of ~4,500 movies and ~470,000 users. The name of each movie is included in a different file. Let's download the dataset and show how the data is structured. We download this dataset from kaggle: https://www.kaggle.com/netflix-inc/netflix-prize-data. In order to use this notebook, you will need to have an account on kaggle as well as a token. \n",
        "\n",
        "From the kaggle API readme[6]: \n",
        ">To use the Kaggle API, sign up for a Kaggle account at https://www.kaggle.com. Then go to the 'Account' tab of your user profile (https://www.kaggle.com/\"username\"/account) [replacing \"username\" with your chosen username] and select 'Create API Token'.This will trigger the download of kaggle.json, a file containing your API credentials. \n",
        "    \n",
        "Open up the json file in your favorite text editor, and copy over the username and key into this notebook.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sRuI0Rt9BeeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "kaggle_username = \"<your username>\"\n",
        "kaggle_key = \"<your token>\"\n",
        "\n",
        "epochs = 500\n",
        "adam_learning_rate = .001\n",
        "adam_beta_1 = 0.9\n",
        "adam_beta_2 = 0.999\n",
        "adam_epsilon = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4l_XuyaCBeee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "token = {\"username\": kaggle_username, \"key\": kaggle_key}\n",
        "\n",
        "!pip3 install --user -q kaggle\n",
        "\n",
        "!mkdir credentials\n",
        "\n",
        "with open('credentials/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "\n",
        "!chmod 600 credentials/kaggle.json\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"./credentials/\"\n",
        "\n",
        "!~/.local/bin/kaggle datasets download netflix-inc/netflix-prize-data -p . --force\n",
        "\n",
        "!mkdir ./netflix-prize-data\n",
        "\n",
        "!unzip -o netflix-prize-data.zip -d ./netflix-prize-data\n",
        "\n",
        "!rm netflix-prize-data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O3CC3BWfBeei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head ./netflix-prize-data/combined_data_1.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWUV0765Been",
        "colab_type": "text"
      },
      "source": [
        "Below we see the distribution of the ratings within the netflix prize dataset. We can see that most of the ratings are 3 or above. This means that overall most users found the movies in this dataset to be 3 stars or more. Our model will likely mimic this distribution of ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Aius8SBeeo",
        "colab_type": "text"
      },
      "source": [
        "![title](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.1/kfp-components/notebooks/recommender_system/assets/dist.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbd9evWgBeep",
        "colab_type": "text"
      },
      "source": [
        "For this notebook we will be using an offical model from tensorflow. We will need to download the repository and then add it to the path. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ma5-uO0JBeeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://github.com/tensorflow/models/archive/master.zip'\n",
        "urllib.request.urlretrieve(url, './models-master.zip')\n",
        "\n",
        "!unzip -o -q models-master.zip\n",
        "!rm models-master.zip\n",
        "\n",
        "sys.path.append('./models-master')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6JPUvtBeev",
        "colab_type": "text"
      },
      "source": [
        "# Matrix Factorization Implementation with Tensorflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F4lzZmzBeew",
        "colab_type": "text"
      },
      "source": [
        "Let's import some of the things that we need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "d5SjVNxkBeex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB_CTMRTBee1",
        "colab_type": "text"
      },
      "source": [
        "We will first have to determine the users and number of movies from the dataset, let's create a function to help us do that. We will create a function that will return a pandas dataframe with the following colums: user_id, movie_id, rating. In the dataset there are 4 files that contain ratings from customer, for this example we will only be using the first of 4 files. In the function, we will first extract all the user ids and ratings, and then we will go about assigning a movie id to each rating as well. \n",
        "\n",
        "\n",
        "We also need a way to reorganize the customer ids that are linear (0-N-1), such that each ID has some movies assocaited with it. After we do that we calculate the number of unqiue movies and unqiue users. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cVNSEINaBee3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataframe(file_paths):\n",
        "    # we create an empty dataframe with 2 columns for the user_id and ratings\n",
        "    df = pd.DataFrame(columns=['user_id', 'rating'])\n",
        "    for file in file_paths:  # we loop through any files we have as arguments\n",
        "        # read the file and append to empty data frame\n",
        "        df_added = pd.read_csv(file, header=None, names=[\n",
        "                               'user_id', 'rating'], usecols=[0, 1])\n",
        "        df = df.append(df_added)\n",
        "    # figure out which indices are nan to determine where the movie ids are\n",
        "    df_nan = pd.DataFrame(pd.isnull(df.rating))\n",
        "    df_nan = df_nan[df_nan['rating'] == True]\n",
        "    df_nan = df_nan.reset_index()\n",
        "    \n",
        "    movie_np = []\n",
        "    movie_id = 0\n",
        "\n",
        "    temp_hold = []\n",
        "    # go through each index of the nans and find which users are associated with a certain movie _id\n",
        "    for i, j in zip(df_nan['index'][1:], df_nan['index'][:-1]):\n",
        "\n",
        "        temp = np.full((i-j-1, 1), movie_id)\n",
        "        temp_hold.append(temp)\n",
        "        movie_id += 1\n",
        "\n",
        "    # add the last record length\n",
        "    last_record = np.full((len(df) - df_nan.iloc[-1, 0] - 1, 1), movie_id)\n",
        "    temp_hold.append(last_record)\n",
        "\n",
        "    movie_np = np.vstack(temp_hold)\n",
        "\n",
        "    # pick only indices from the original dataframe that are not nan\n",
        "    df = df[pd.notnull(df['rating'])]\n",
        "\n",
        "    # add the movie_id column to the dataframe\n",
        "    df['movie_id'] = movie_np.astype(int)\n",
        "    df['user_id'] = df['user_id'].astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "file_paths = ['./netflix-prize-data/combined_data_1.txt', './netflix-prize-data/combined_data_2.txt',\n",
        "              './netflix-prize-data/combined_data_3.txt', './netflix-prize-data/combined_data_4.txt']\n",
        "\n",
        "files = ['./netflix-prize-data/combined_data_1.txt']\n",
        "\n",
        "df = create_dataframe(files)\n",
        "\n",
        "user_ids = df['user_id'].values\n",
        "\n",
        "unique_ids = np.unique(user_ids)\n",
        "\n",
        "id_dict = {id: counter for counter, id in enumerate(unique_ids)}\n",
        "\n",
        "df['user_id'] = df['user_id'].apply(lambda x: id_dict[x])\n",
        "\n",
        "num_movies = np.unique(df['movie_id'].values).size\n",
        "num_users = unique_ids.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__9vj_VgBee7",
        "colab_type": "text"
      },
      "source": [
        "# Create Matrix Factorization Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4WZ50JSBee8",
        "colab_type": "text"
      },
      "source": [
        "For the model we will need to define parameters with a dictionary that we will feed into the model. These parameters include information for training hyperparemeters, layer sizes, regulation, optimization hyperparameters, etc. We can play around with these parameters to influence the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PzPQdUBmBee9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = \\\n",
        "    {\n",
        "        \"train_epochs\": epochs,\n",
        "        \"batches_per_step\": 1,\n",
        "        \"use_seed\": False,\n",
        "        \"batch_size\": 10000,\n",
        "        \"eval_batch_size\": 1,\n",
        "        \"learning_rate\": adam_learning_rate,\n",
        "        \"mf_dim\": 10,  # this is number of latent dimensions, basically how we describe each movie & user\n",
        "        \"model_layers\": [int(layer) for layer in [\"64\", \"32\", \"16\", \"8\"]],\n",
        "        \"mf_regularization\": 0.0,\n",
        "        \"mlp_reg_layers\": [float(reg) for reg in [\"0.\", \"0.\", \"0.\", \"0.\"]],\n",
        "        \"num_neg\": 4,\n",
        "        \"num_gpus\": 0,\n",
        "        \"use_tpu\": False,\n",
        "        \"tpu\": None,\n",
        "        \"tpu_zone\": None,\n",
        "        \"tpu_gcp_project\": None,\n",
        "        \"beta1\": adam_beta_1,\n",
        "        \"beta2\": adam_beta_2,\n",
        "        \"epsilon\": adam_epsilon,\n",
        "        \"match_mlperf\": False,\n",
        "        \"use_xla_for_gpu\": False,\n",
        "        \"clone_model_in_keras_dist_strat\":False,\n",
        "        \"epochs_between_evals\": 1,\n",
        "        \"turn_off_distribution_strategy\": True,\n",
        "        \"num_users\": num_users,\n",
        "        \"num_items\": num_movies,\n",
        "        \"loss\": 'mse',\n",
        "        \"train_size\": 0.95  # split between\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8zUAD-NBefC",
        "colab_type": "text"
      },
      "source": [
        "After we generate our parameters dictionary, we have to create the model we will use for matrix factorization. The model that we will use is from the tensorflow offical models for recommendation engines. This model and all of the dependencies can be downloaded from: https://github.com/tensorflow/models. We will be using the tf.keras interface for the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1GnSFx30BefD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from official.recommendation import neumf_model\n",
        "\n",
        "def create_model(params):\n",
        "    user_input = tf.keras.layers.Input(\n",
        "        shape=(),\n",
        "        batch_size=None,\n",
        "        name='user_id',\n",
        "        dtype=tf.int32)\n",
        "\n",
        "    item_input = tf.keras.layers.Input(\n",
        "        shape=(),\n",
        "        batch_size=None,\n",
        "        name='movie_id',\n",
        "        dtype=tf.int32)\n",
        "\n",
        "    base_model = neumf_model.construct_model(\n",
        "        user_input, item_input, params, need_strip=False)\n",
        "\n",
        "    rating = base_model.output\n",
        "\n",
        "    keras_model = tf.keras.Model(\n",
        "        inputs=[user_input, item_input],\n",
        "        outputs=rating)\n",
        "    return keras_model\n",
        "\n",
        "model = create_model(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMO56Hx1BefH",
        "colab_type": "text"
      },
      "source": [
        "We have to prepare the dataframe that we have for training. We need to create 3, 1-dimensional tensors that will hold information about users, movies, and the related ratings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "V2P_RRB2BefI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_ratings = df.shape[0]\n",
        "\n",
        "ratings = np.concatenate((np.array(df['user_id'], dtype=pd.Series).reshape(num_ratings, 1),\n",
        "                          np.array(df['movie_id'], dtype=pd.Series).reshape(\n",
        "                              num_ratings, 1),\n",
        "                          np.array(df['rating'], dtype=pd.Series).reshape(num_ratings, 1)), axis=1)\n",
        "ratings = ratings.astype(np.float64)\n",
        "\n",
        "ratings_tr, ratings_val = train_test_split(\n",
        "    ratings, train_size=params['train_size'])\n",
        "\n",
        "user_tr, movie_tr, rating_tr = ratings_tr[:, 0].T.astype(np.int32).reshape(-1,), \\\n",
        "    ratings_tr[:, 1].T.astype(np.int32).reshape(-1,), \\\n",
        "    ratings_tr[:, 2].T\n",
        "\n",
        "user_val, movie_val, rating_val = ratings_val[:, 0].T.astype(np.int32).reshape(-1,), \\\n",
        "    ratings_val[:, 1].T.astype(np.int32).reshape(-1,), \\\n",
        "    ratings_val[:, 2].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQDNYdFKBefM",
        "colab_type": "text"
      },
      "source": [
        "In the next code block we will define three different metrics, precision, recall, and F1 score for our model. To generate these metrics, we need to also define recommended items and relevant items. A relevant item is when the actual rating is above a threshold (for us it will be 3.5). A recommended item is when we predict the rating to be above a certain threshold (again 3.5). Precision, or positive predictive value, is the proportion of recommended items that are relevant. Recall, or sensitivity, is the proportion of relevant items found among the recommended items. F1 score is the harmonic mean between precision and recall. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MLPJaPNQBefN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = tf.constant(3.5, dtype=tf.float32)\n",
        "checker = tf.constant(2.0, dtype=tf.float32)\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    relevant = tf.cast(y_true >= threshold, dtype=tf.float32)\n",
        "    recommended = tf.cast(y_pred >= threshold, dtype=tf.float32)\n",
        "\n",
        "    relevant_recommend = tf.reduce_sum(tf.cast(tf.reshape(\n",
        "        relevant + recommended, (-1, 1)) >= checker, dtype=tf.float32))\n",
        "\n",
        "    return tf.divide(relevant_recommend, tf.reduce_sum(recommended))\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "\n",
        "    relevant = tf.cast(y_true >= threshold, dtype=tf.float32)\n",
        "    recommended = tf.cast(y_pred >= threshold, dtype=tf.float32)\n",
        "\n",
        "    relevant_recommend = tf.reduce_sum(tf.cast(tf.reshape(\n",
        "        relevant + recommended, (-1, 1)) >= checker, dtype=tf.float32))\n",
        "    return tf.divide(relevant_recommend, tf.reduce_sum(relevant))\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    # harmonic mean of precision & recall\n",
        "    def precision(y_true, y_pred):\n",
        "        relevant = tf.cast(y_true >= threshold, dtype=tf.float32)\n",
        "        recommended = tf.cast(y_pred >= threshold, dtype=tf.float32)\n",
        "\n",
        "        relevant_recommend = tf.reduce_sum(tf.cast(tf.reshape(\n",
        "            relevant + recommended, (-1, 1)) >= checker, dtype=tf.float32))\n",
        "\n",
        "        return tf.divide(relevant_recommend, tf.reduce_sum(recommended))\n",
        "\n",
        "    def recall(y_true, y_pred):\n",
        "\n",
        "        relevant = tf.cast(y_true >= threshold, dtype=tf.float32)\n",
        "        recommended = tf.cast(y_pred >= threshold, dtype=tf.float32)\n",
        "\n",
        "        relevant_recommend = tf.reduce_sum(tf.cast(tf.reshape(\n",
        "            relevant + recommended, (-1, 1)) >= checker, dtype=tf.float32))\n",
        "        return tf.divide(relevant_recommend, tf.reduce_sum(relevant))\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "\n",
        "    return 2*(precision*recall)/(precision + recall)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-glpFVRQBefR",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsvA2grLBefS",
        "colab_type": "text"
      },
      "source": [
        "Below we will compile and train our model. We have callbacks for model checkpoint and early stopping. This will make sure that the model with the best performance on the validation set is saved, and that we stop training when the performance on the validation stagnates. The history of our training will be saved, and then used later to visualize our performance on the training and validation sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fKgcRjCsBefT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    lr=params[\"learning_rate\"],\n",
        "    beta_1=params[\"beta1\"],\n",
        "    beta_2=params[\"beta2\"],\n",
        "    epsilon=params[\"epsilon\"])\n",
        "\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    './model.h5', monitor='val_loss', save_best_only=True, verbose=0, mode='auto', period=1)\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, verbose=0)\n",
        "\n",
        "model.compile(\n",
        "    loss=params['loss'],\n",
        "    metrics=[precision, recall, f1],\n",
        "    optimizer=optimizer)\n",
        "\n",
        "history = model.fit(\n",
        "    x=(user_tr, movie_tr), y=rating_tr,\n",
        "    epochs=params['train_epochs'],\n",
        "    verbose=1,\n",
        "    batch_size=params['batch_size'],\n",
        "    validation_data=([user_val, movie_val], rating_val),\n",
        "    callbacks=[callback, earlystopping])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKdEM_oJBefX",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJlDtGgjBefZ",
        "colab_type": "text"
      },
      "source": [
        "After we train the model, let's delete the most recent model from memory and let's load the model that had the best perfomance on the validation set. We will load the model with our custom metrics. We will also generate some metrics on the validation set using our loaded model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RYBTU9m7Befa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "\n",
        "model = tf.keras.models.load_model('model.h5', custom_objects={\n",
        "                                   'precision': precision, 'recall': recall, 'f1': f1})\n",
        "\n",
        "names = model.metrics_names\n",
        "\n",
        "loss_and_metrics = model.evaluate(\n",
        "    x=[user_val, movie_val], y=rating_val, batch_size=params['batch_size'], verbose=0)\n",
        "\n",
        "for name, value in zip(names, loss_and_metrics):\n",
        "    print('{0:s} : {1:1.4f}'.format(name, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2LDbVJBefh",
        "colab_type": "text"
      },
      "source": [
        "Below we will visualize the following\n",
        "\n",
        "- Loss curve on the train and validation set. \n",
        "\n",
        "- Precision curve on the train and validation set.\n",
        "\n",
        "- Recall curve on the train and validation set.\n",
        "\n",
        "- F1 curve on the train and validation set. \n",
        "\n",
        "We will see that for the validation set, the precision, recall, and F1 curves are rather noisy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2ZkQDLz4Befj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "hist_keys = ['loss', 'precision', 'recall', 'f1', 'val_loss', 'val_precision',  'val_recall', 'val_f1']\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for plot_num, train, val in zip(range(1, 5), hist_keys[0:4], hist_keys[4:]):\n",
        "    plt.subplot(4, 1, plot_num)\n",
        "    plt.plot(history.history[train])\n",
        "    plt.plot(history.history[val])\n",
        "    plt.title(train.capitalize() + ' curve')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(train.capitalize())\n",
        "    plt.legend(['training', 'validation'])\n",
        "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.5, hspace=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QlTTa1UBefn",
        "colab_type": "text"
      },
      "source": [
        "# Interpret Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAEbW9BxBefo",
        "colab_type": "text"
      },
      "source": [
        "As we noted earlier, our model will likely mimic the distribution of the dataset at large. Below we will plot the distribution of predicted ratings in our validation dataset. In the red, we will see the true distribution of our dataset, and the blue will represent the predicted distribution of our dataset. We see that they match up closely, but our model is hesistant to apply extreme ratings (1 or 5) to inputted data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ODbmt0qzBefp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "predicted_ratings = model.predict([user_val, movie_val])\n",
        "predicted_ratings = np.round(predicted_ratings[(np.round(predicted_ratings) <= 5) & (np.round(predicted_ratings) >= 1)])\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "plt.subplot(2, 1, 1)\n",
        "sns.countplot(rating_val, color = 'r', saturation = 1)\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Rating Distribution of Actual Ratings')\n",
        "plt.subplot(2, 1, 2)\n",
        "sns.countplot(predicted_ratings.flatten(),color = 'b', saturation = 1)\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Rating Distribution of Predicted Ratings')\n",
        "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.5, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdwPyISLBeft",
        "colab_type": "text"
      },
      "source": [
        "After we have created and fit the model, we need to have another method of visually seeing how close our model got to approximating the ratings that users gave to specific movies. We use a pandas dataframe in order to showcase some comparisons between actual ratings and our predicted ratings. We will only use the users and movies from the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KcoUyMImBefu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_list_comparison(model, user_val, movie_val, rating_val, num=5):\n",
        "\n",
        "    inds = np.random.randint(0, high=rating_val.size, size=num)\n",
        "    df = pd.DataFrame(columns=[\n",
        "                      \"User ID\", \"Movie ID\", \"Actual Rating\", \"Predicted Rating\", \"Rating Difference\"])\n",
        "\n",
        "    for ind in inds:\n",
        "        usr = user_val[ind].astype(np.int64)\n",
        "        mov = movie_val[ind].astype(np.int64)\n",
        "        df = df.append({\"User ID\": usr, \"Movie ID\": mov, \"Actual Rating\": rating_val[ind], \"Predicted Rating\": model.predict([usr.reshape(1,), mov.reshape(1,)])[\n",
        "                       0][0], \"Rating Difference\": np.abs(model.predict([usr.reshape(1,), mov.reshape(1,)])[0][0] - rating_val[ind])}, ignore_index=True)\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "\n",
        "print_list_comparison(model, user_val, movie_val, rating_val, num=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJl6qGsZBefx",
        "colab_type": "text"
      },
      "source": [
        "To further assess the model, we will show what movies a user hated and loved (shown by a rating of 1 to 5 respectively), and then show a graphical comparison between the predicted ratings and actual ratings. We will use the movies that have the highest number of reviewed users in order to make sure our matrix has non empty cells. Dark blue is assoicated with a rating of 5, and a light color of blue is associated with a rating of 1. In the final matrix, a light color of blue is associated with a small difference in the actual and predicted rating, while a dark color of blue is associated with a large diffrence in the ratings. If the difference in ratings is all light blue then we can consider our performance to have generalized the preferences of the users. Again here, we will only use users and movies from the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GGbmiUCuBefz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def movie_matrix_comparisons(model, df, movies=10, users=10):\n",
        "    movie_viewers = df['movie_id'].value_counts()[0:movies]\n",
        "    movies_ = list(movie_viewers.index)\n",
        "    num_user = list(movie_viewers.values)\n",
        "\n",
        "    user_set = []\n",
        "    for movie in movies_:\n",
        "        user_set.append(\n",
        "            set(df.loc[df['movie_id'] == movie].iloc[:, 0].values.flatten().tolist()))\n",
        "\n",
        "    cross_users = list(set.intersection(*user_set))\n",
        "\n",
        "    inds = np.random.randint(0, high=len(cross_users), size=users)\n",
        "\n",
        "    users_ = [cross_users[ind] for ind in inds]\n",
        "\n",
        "    true_mat = np.zeros((movies, users))\n",
        "    pred_mat = np.zeros((movies, users))\n",
        "\n",
        "    for i, user in enumerate(users_):\n",
        "        for j, movie in enumerate(movies_):\n",
        "            true_mat[i, j] = df.loc[(df['movie_id'] == movie) & (\n",
        "                df['user_id'] == user)]['rating'].values\n",
        "            pred_mat[i, j] = model.predict(\n",
        "                [np.array(user).reshape(1,), np.array(movie).reshape(1,)])\n",
        "\n",
        "    fig, (ax0, ax1, ax2) = plt.subplots(3, 1, figsize=(20, 10))\n",
        "\n",
        "    c = ax0.pcolor(true_mat, cmap='Blues', vmin=1.0, vmax=5.0)\n",
        "    ax0.set_title('Actual Ratings')\n",
        "    ax0.set_ylabel('User')\n",
        "    ax0.set_xlabel('Movies')\n",
        "    fig.colorbar(c, ax=ax0)\n",
        "\n",
        "    c = ax1.pcolor(pred_mat, cmap='Blues', vmin=1.0, vmax=5.0)\n",
        "    ax1.set_title('Predicted Ratings')\n",
        "    ax1.set_ylabel('User')\n",
        "    ax1.set_xlabel('Movies')\n",
        "    fig.colorbar(c, ax=ax1)\n",
        "\n",
        "    c = ax2.pcolor(np.abs(pred_mat - true_mat),\n",
        "                   cmap='Blues', vmin=0.0, vmax=5.0)\n",
        "    ax2.set_title('Difference Between Ratings')\n",
        "    ax2.set_ylabel('User')\n",
        "    ax2.set_xlabel('Movies')\n",
        "    fig.colorbar(c, ax=ax2)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return true_mat, pred_mat\n",
        "\n",
        "\n",
        "true_mat, pred_mat = movie_matrix_comparisons(model, df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ab0rMG5Bef3",
        "colab_type": "text"
      },
      "source": [
        "Finally, we want to see how our model compares to simply averaging the rating for each movie. This is done as a sanity check to make sure that our model actually tries to achieve our goal. We will numerically check the errors between the averaged, predicted, and true ratings. We should see that the error in the averaged ratings is larger than in the predicted ratings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NMqMv5k3Bef4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_comp(model, df, movies=10, users=10):\n",
        "    \n",
        "    num_ratings = movies*users\n",
        "    \n",
        "    movie_viewers = df['movie_id'].value_counts()[0:movies]\n",
        "    movies_ = list(movie_viewers.index)\n",
        "    num_user = list(movie_viewers.values)\n",
        "\n",
        "    user_set = []\n",
        "    for movie in movies_:\n",
        "        user_set.append(\n",
        "            set(df.loc[df['movie_id'] == movie].iloc[:, 0].values.flatten().tolist()))\n",
        "\n",
        "    cross_users = list(set.intersection(*user_set))\n",
        "\n",
        "    inds = np.random.randint(0, high=len(cross_users), size=users)\n",
        "\n",
        "    users_ = [cross_users[ind] for ind in inds]\n",
        "\n",
        "    true_mat = np.zeros((movies, users))\n",
        "    pred_mat = np.zeros((movies, users))\n",
        "    ave_mat = np.zeros((movies, users))\n",
        "\n",
        "    for i, user in enumerate(users_):\n",
        "        for j, movie in enumerate(movies_):\n",
        "            true_mat[i, j] = df.loc[(df['movie_id'] == movie) & (\n",
        "                df['user_id'] == user)]['rating'].values\n",
        "            pred_mat[i, j] = model.predict(\n",
        "                [np.array(user).reshape(1,), np.array(movie).reshape(1,)])\n",
        "            ave_mat[i, j] = np.mean(df.loc[df['movie_id'] == movie]['rating'].values)\n",
        "\n",
        "    print('Error between Average and Real: {}'.format(np.sum(np.abs(ave_mat - true_mat))/num_ratings))\n",
        "    print('Error between Predicted and Real: {}'.format(np.sum(np.abs(pred_mat - true_mat))/num_ratings))\n",
        "\n",
        "average_comp(model, df, movies = 100, users = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1-YZnm5Bef8",
        "colab_type": "text"
      },
      "source": [
        "# References \n",
        "1. Matrix Factorization Techniques for Recommedner Systems, Koren, Yahoo Research, Robert Bell and Chris Volinsky, AT&T Labs—Research\n",
        "2. Cichocki, Andrzej, and P. H. A. N. Anh-Huy. “Fast local algorithms for large scale nonnegative matrix and tensor factorizations.” IEICE transactions on fundamentals of electronics, communications and computer sciences 92.3: 708-721, 2009.\n",
        "3. Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9).\n",
        "4. http://www.pitt.edu/~peterb/AIS2013/CollaborativeFiltering.pdf\n",
        "5. http://kojinoshiba.com/recsys-cold-start/\n",
        "6. https://github.com/Kaggle/kaggle-api/"
      ]
    }
  ]
}